{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpeH3iWE7dQiXWVVuiiK+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saketharam1/11239A014-nlp_observation/blob/main/nlp_observation14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "sentence = \"The cats are running faster than the dogs\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "print(\"Lemmatized words:\")\n",
        "for word in words:\n",
        "    lemma = lemmatizer.lemmatize(word)\n",
        "    print(f\"{word} --> {lemma}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39154945-1871-4cb0-8432-5b244085a351",
        "id": "94UJNd-vWEXY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized words:\n",
            "The --> The\n",
            "cats --> cat\n",
            "are --> are\n",
            "running --> running\n",
            "faster --> faster\n",
            "than --> than\n",
            "the --> the\n",
            "dogs --> dog\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "text = \"NLP is my favourite subject.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\")\n",
        "print(sentences)\n",
        "words = word_tokenize(text)\n",
        "print(\"\\nWord Tokenization:\")\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1OGQjqXxa_X",
        "outputId": "af00eb76-28ee-48ba-9280-cc7f91ab2f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "['NLP is my favourite subject.']\n",
            "\n",
            "Word Tokenization:\n",
            "['NLP', 'is', 'my', 'favourite', 'subject', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "stemmer = PorterStemmer()\n",
        "sentence = \"cats running faster than the other animals.\"\n",
        "words = word_tokenize(sentence)\n",
        "print(\"Original Sentence:\")\n",
        "print(sentence)\n",
        "print(\"\\nStemmed Words:\")\n",
        "for word in words:\n",
        "    print(f\"{word} --> {stemmer.stem(word)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9Iu-M2h0KaA",
        "outputId": "847cdd92-c6cc-4a0e-f1ad-7a96a600aef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence:\n",
            "cats running faster than the other animals.\n",
            "\n",
            "Stemmed Words:\n",
            "cats --> cat\n",
            "running --> run\n",
            "faster --> faster\n",
            "than --> than\n",
            "the --> the\n",
            "other --> other\n",
            "animals --> anim\n",
            ". --> .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.stem import PorterStemmer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stemmer = PorterStemmer()\n",
        "sentence = \"The children are playing happily with their friends.\"\n",
        "doc = nlp(sentence)\n",
        "print(\"Word - Lemma - Stem - POS - Morphological Features\\n\")\n",
        "for token in doc:\n",
        "    stem = stemmer.stem(token.text)\n",
        "    print(f\"{token.text:10} | {token.lemma_:10} | {stem:10} | {token.pos_:10} | {token.morph}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdaW6r0e3Ebp",
        "outputId": "0080975e-f7a0-40ca-9831-f0a3ea5c718b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word - Lemma - Stem - POS - Morphological Features\n",
            "\n",
            "The        | the        | the        | DET        | Definite=Def|PronType=Art\n",
            "children   | child      | children   | NOUN       | Number=Plur\n",
            "are        | be         | are        | AUX        | Mood=Ind|Tense=Pres|VerbForm=Fin\n",
            "playing    | play       | play       | VERB       | Aspect=Prog|Tense=Pres|VerbForm=Part\n",
            "happily    | happily    | happili    | ADV        | \n",
            "with       | with       | with       | ADP        | \n",
            "their      | their      | their      | PRON       | Number=Plur|Person=3|Poss=Yes|PronType=Prs\n",
            "friends    | friend     | friend     | NOUN       | Number=Plur\n",
            ".          | .          | .          | PUNCT      | PunctType=Peri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "text = \"I am lernig Python and it is amazng\"\n",
        "blob = TextBlob(text)\n",
        "corrected_text = blob.correct()\n",
        "\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Corrected Text:\", corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs5-FZlT3bLQ",
        "outputId": "2f28e88f-2506-4cf0-d11e-467417353f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:  I am lernig Python and it is amazng\n",
            "Corrected Text: I am leaning Python and it is amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "nlp_infer = pipeline(\"text-classification\", model=\"facebook/bart-large-mnli\")\n",
        "premise = \"All humans are mortal. Socrates is a human.\"\n",
        "hypothesis = \"Socrates is mortal.\"\n",
        "result = nlp_infer(f\"{premise} </s> {hypothesis}\")\n",
        "print(\"Premise:   \", premise)\n",
        "print(\"Hypothesis:\", hypothesis)\n",
        "print(\"\\nDeduction Result:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWrx_dbi5e9J",
        "outputId": "cd05b14f-6e69-4258-a376-58252ec8ce5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Premise:    All humans are mortal. Socrates is a human.\n",
            "Hypothesis: Socrates is mortal.\n",
            "\n",
            "Deduction Result: [{'label': 'entailment', 'score': 0.9919486045837402}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import tee\n",
        "corpus = \"I love natural language processing and I love Python programming\"\n",
        "words = corpus.lower().split()\n",
        "\n",
        "# Function to generate n-grams\n",
        "def ngrams(tokens, n):\n",
        "    return zip(*[tokens[i:] for i in range(n)])\n",
        "\n",
        "# Count unigrams, bigrams, trigrams\n",
        "unigrams = Counter(words)\n",
        "bigrams = Counter(ngrams(words, 2))\n",
        "trigrams = Counter(ngrams(words, 3))\n",
        "\n",
        "total_words = sum(unigrams.values())\n",
        "total_bigrams = sum(bigrams.values())\n",
        "total_trigrams = sum(trigrams.values())\n",
        "\n",
        "# Print probabilities\n",
        "print(\"Unigrams:\")\n",
        "for w, c in unigrams.items():\n",
        "    print(f\"{w:10} : {c/total_words:.3f}\")\n",
        "\n",
        "print(\"\\nBigrams:\")\n",
        "for bg, c in bigrams.items():\n",
        "    print(f\"{bg} : {c/total_bigrams:.3f}\")\n",
        "\n",
        "print(\"\\nTrigrams:\")\n",
        "for tg, c in trigrams.items():\n",
        "    print(f\"{tg} : {c/total_trigrams:.3f}\")\n"
      ],
      "metadata": {
        "id": "bSEn4opAERnL",
        "outputId": "79398903-3093-40be-da4e-cdb1de1e58de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams:\n",
            "i          : 0.200\n",
            "love       : 0.200\n",
            "natural    : 0.100\n",
            "language   : 0.100\n",
            "processing : 0.100\n",
            "and        : 0.100\n",
            "python     : 0.100\n",
            "programming : 0.100\n",
            "\n",
            "Bigrams:\n",
            "('i', 'love') : 0.222\n",
            "('love', 'natural') : 0.111\n",
            "('natural', 'language') : 0.111\n",
            "('language', 'processing') : 0.111\n",
            "('processing', 'and') : 0.111\n",
            "('and', 'i') : 0.111\n",
            "('love', 'python') : 0.111\n",
            "('python', 'programming') : 0.111\n",
            "\n",
            "Trigrams:\n",
            "('i', 'love', 'natural') : 0.125\n",
            "('love', 'natural', 'language') : 0.125\n",
            "('natural', 'language', 'processing') : 0.125\n",
            "('language', 'processing', 'and') : 0.125\n",
            "('processing', 'and', 'i') : 0.125\n",
            "('and', 'i', 'love') : 0.125\n",
            "('i', 'love', 'python') : 0.125\n",
            "('love', 'python', 'programming') : 0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def ngram_smooth(text, n):\n",
        "    tokens = text.split()\n",
        "    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "    counts = Counter(ngrams)\n",
        "    prefix_counts = Counter([ng[:-1] for ng in ngrams])\n",
        "    V = len(set(tokens))\n",
        "    return {ng: (counts[ng]+1)/ (prefix_counts[ng[:-1]] + V)for ng in counts}\n",
        "\n",
        "# Example\n",
        "text = \"the bird is flying\"\n",
        "probs = ngram_smooth(text, 2)\n",
        "print(probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_b1tM-LTSbi",
        "outputId": "70c2abbd-7b85-4f77-c9bc-f6aeeb255e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('the', 'bird'): 0.4, ('bird', 'is'): 0.4, ('is', 'flying'): 0.4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(\"Tokens and their POS tags:\")\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"{word:10} --> {tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI0jaoRrTct3",
        "outputId": "5a6a9245-ba20-468a-8c43-1786477059a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens and their POS tags:\n",
            "The        --> DT\n",
            "quick      --> JJ\n",
            "brown      --> NN\n",
            "fox        --> NN\n",
            "jumps      --> VBZ\n",
            "over       --> IN\n",
            "the        --> DT\n",
            "lazy       --> JJ\n",
            "dog        --> NN\n",
            ".          --> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "nltk.download('treebank'); nltk.download('universal_tagset')\n",
        "\n",
        "# Load data\n",
        "data = treebank.tagged_sents(tagset='universal')\n",
        "train, test = data[:3500], data[3500:]\n",
        "\n",
        "# Build taggers\n",
        "default = nltk.DefaultTagger('NOUN')\n",
        "uni = nltk.UnigramTagger(train, backoff=default)\n",
        "bi = nltk.BigramTagger(train, backoff=uni)\n",
        "\n",
        "# Blend function (prefer bigram if available, else unigram)\n",
        "def blend_tag(sentence):\n",
        "    uni_tags = uni.tag(sentence)\n",
        "    bi_tags = bi.tag(sentence)\n",
        "    blended = []\n",
        "    for (word, bi_tag), (_, uni_tag) in zip(bi_tags, uni_tags):\n",
        "        blended.append((word, bi_tag if bi_tag is not None else uni_tag))\n",
        "    return blended\n",
        "\n",
        "# Test\n",
        "sent = [\"The\", \"cat\", \"sits\", \"on\", \"the\", \"mat\"]\n",
        "blended_tags = blend_tag(sent)\n",
        "\n",
        "print(\"Blended POS Tags:\")\n",
        "for word, tag in blended_tags:\n",
        "    print(f\"{word:10} --> {tag}\")\n",
        "\n",
        "print(\"Accuracy:\", bi.evaluate(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbURT5lyTkVC",
        "outputId": "d74698ce-0e37-41e9-c906-21a061f5fd26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended POS Tags:\n",
            "The        --> DET\n",
            "cat        --> NOUN\n",
            "sits       --> NOUN\n",
            "on         --> ADP\n",
            "the        --> DET\n",
            "mat        --> NOUN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2272760284.py:31: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print(\"Accuracy:\", bi.evaluate(test))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9391034950140381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tag import hmm\n",
        "nltk.download('treebank')\n",
        "data = treebank.tagged_sents()\n",
        "train_data = data[:3000]\n",
        "test_data = data[3000:]\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "hmm_tagger = trainer.train_supervised(train_data)\n",
        "sentence = [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\"]\n",
        "tags = hmm_tagger.tag(sentence)\n",
        "print(\"HMM POS Tagging:\")\n",
        "for word, tag in tags:\n",
        "    print(f\"{word:10} --> {tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKK3j3sLTtGX",
        "outputId": "e525980a-47f9-46f9-e919-3107eb0870db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:333: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:335: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:331: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMM POS Tagging:\n",
            "The        --> DT\n",
            "quick      --> JJ\n",
            "brown      --> NNP\n",
            "fox        --> NNP\n",
            "jumps      --> NNP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:363: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        }
      ]
    }
  ]
}